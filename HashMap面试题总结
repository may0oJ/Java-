1. HashMap的数据结构/说下HashMap
答：HashMap由基础的数组+链表构成，当链表过长时会转化为红黑树避免链条过长。数组是HashMap的主体用于存储拥有对应下标的entry，
链表则主要是解决hash collision而存在的，即entry下标相同的情况。

2.HashMap的工作原理
答：HashMap底层是由数组和链表构成，其中的每个元素以键值对的形式存在，其由Node内部类实现（实现的是Map.Entry<K,V>接口）。HashMap最常用
的也是其中包含核心内容最多的部分是其put & get 方法，分别实现存储和获取。
put方法的基本流程是，存储对象时：
1.将K/V键值传入put()方法；
2.首先检查数组是否初始化，若尚未初始化则初始化一个哈希数组（这里可能会调用resize方法）；
2.5如果Key为null，则调用PutForNullKey（value）方法，将其放到index为0的桶的位置，如果table[0]的链表已存在键为null的元素，则将新value赋值给它；没有则创建新Entry。
3.将Key的hashcode通过hash函数（扰动函数，1.7中4次位运算4次异或，1.8中hashcode前16位与后16位进行异或运算）得到其对应的hash值，然后通过indexFor
方法将hash值与当前数组的长度-1进行&运算，得到的即为其对应的存储的地址。
4.检查该下标的位置是否存在键，若已存在，则遍历链表检查是否有与该键相同的entry存在，若有则直接覆盖为新的value；若遍历链表发现没有相同的key的entry，则在链表尾部建立新的
entry（1.7采用头插法，但在多个线程同时插入一个entry的时候若同时发生了扩容，则有可能会导致出现环形链表；1.8改用尾插法从而避免了这种情况的发生）。
4.5在插入时同时还会检查当前是否为红黑树结构，若是则会采用树的插入方法。
5.在插入之后会检查当前链表上的元素数量是否达到了8，若达到则会将链表转换为红黑树。
6.之后会检查当前hashmap中的元素个数即size是否已经超过了loadfacotr*capacity(初始化时默认为0.75,16；也可以自定义传参数进去；自己设定的capacity会通过tableSizeFor()方法获得
在其之上最接近的2的整数次幂作为实际capacity，1除外)，若已超过会调用resize方法。
7.操作结束后最终返回被覆盖的value，或是null表示没有对应的Key存在，也有可能是之前的Key对应的键值为null。

get方法的基本流程是，获取对象时：
1.将Key的hashcode传入hash函数，通过hash函数计算key的hash值，计算index，前往对应index的桶的位置；
2.遍历链表或树，检查是否有相同的Key，若有返回该Key对应的value，没有返回null，但也有可能对应的value就是null(因为HashMap允许存在多个null值).

3.当两个对象的hashcode相同时会发生什么？
答：当hashcode相同时，在hashmap中计算桶的下标就会计算得出同样的结果，也就发生了哈希碰撞；但是hashcode相同并不一定两者就相等，若比较的两个对象不同，那么会插入并写入新的Entry；
equlas()方法默认比较的是两者是否为同一对象，即实际内存中的地址，这也是为什么使用自创类型的Key/Value时需要覆写equals方法。

4.你知道hash()的实现吗？为什么要这么实现？为什么使用异或运算？
答：在jdk 1.7中，需要对对象的hashcode进行4次位运算和4次异或运算，而在jdk 1.8中，改成了只需1次位运算和1次异或运算，具体方法是对32位的hashcode向右移16位再与原本的hashcode进行
异或运算，这次改进的主要目的是降低运算量从而提高效率、减少系统开销，可以看出计算方法精简了很多，但得出的结果也足够的满足散列均匀。
hash函数的两大特性就是要求计算简单并且可以达到散列地址分布均匀的效果。异或运算是位运算，它的效率要比其他运算高很多，例如hashtable中计算hash值就是用的取模运算，其效率相较于位运算
低很多，主要原因是位运算直接对内存数据进行操作，不需要转换成十进制，因此处理速度很快；同时因为异或运算的特性，hashcode中任何一位改变都会造成计算结果的改变，从而实现散布均匀。

5.HashMap的table的容量如何确定？loadFactor是什么？该容量如何变化？这种变化会带来什么问题？
答：table容量是由capacity参数来确定的，构建hashmap时如不指定capacity会默认是16，也可以自己指定capacity，但会经过tableSizeFor()方法向上取到与其最接近的2的整数次幂，1除外，得到的
结果还是1；capacity的范围是1<=cap<=1<<30;
loadfactor是装载因子，默认是0.75，也可自己指定其使用方法是用当前的capacity*loadfactor，如果当前的size大于了这个值，那么就会触发resize方法进行扩容；

扩容会将新容量变为之前的2倍，即array的长度变为2倍，之后要重新计算map中各个key对应的桶下标，具体方法为若key的hash值之前取到的最高位的前一位是0的话，
那么地址不变；如果是1的话，那么新地址就是原地址+旧容量大小；如果数据很大的情况下，扩容时将会带来性能的损失，如果在性能要求很高的情况下，可能会造成重大的影响。

6.HashMap的遍历方法及其特点
答：
a.for each map.keyset() 当只需要k值时，推荐使用
b.for each map.entryset() 当需要v值及k值时，推荐使用
c.for each map.values() 当只需要v值时，推荐使用
d.map.entrySet().iterator() 通过Map.entrySet使用iterator遍历key和value

7.HashMap, LinkedHashMap, TreeMap有什么区别？
答:HashMap中的key值存储是根据hash计算出的地址随机存储的，因此是无序的；而LinkdeHashMap和TreeMap都是有序的，其中，LinkedHashMap内部通过给每个node添加after和before属性
使其内部维护了一个双向链表，使其可以根据插入顺序或者访问顺序进行遍历；而TreeMap是按照key的自然顺序或者comparator定义的排序规则进行排序的，内部是通过红黑树实现的。
HashMap和LinkedHashMap都支持一个键为null，多个值为null；TreeMap支持值为null，但键为null由于其比较器自身的问题，例如当key类型为String时我们默认用的是String类的compareTo()
方法，而这时如果写入一个键值对其中Key为null，会报出NullPointerException;虽然也可以自己传入比较器已实现可以存储null键，但使用get()方法获取其value的时候仍会出现问题，因此一般
不用TreeMap存储null键。
HashMap和LinkedHashMap中put和get等方法的时间复杂度都可视为O(1),而TreeMap中的get，put，containsKey，remove操作时间复杂度都是O(log(n))；
HashMap的遍历速度与其数组的长度以及键值对的实际数量有关，而LinkedHashMap的遍历速度只与其实际数据量有关。

8.HashMap, LinkedHashMap, TreeMap使用场景？
答：一般情况下，当对顺序没有要求只是为了实现键值对的插入删除查找等操作时优先使用HashMap；
当对保持插入顺序或者访问顺序有需求时，使用LinkedHashMap；
当需要按照自然顺序或是自定义的比较规则遍历键的时候使用TreeMap。

9.HashMap和HashTable的区别？
答：
a.HashTable使用了synchronized字段修饰方法，因此是线程安全的；但HashMap线程不安全；
b.HashMap支持一个Null键多个Null值，但HashTable不支持任何Null为键或者值，会抛出NullPointerException；
c.HashMap初始默认大小为16，loadfactor为0.75，每次扩容为之前的2倍，或通过tableSizeFor()方法将传入的初始capacity向上取到最近接的2的整数次幂的容量，
HashTable初始默认大小为11，loadfactor为0.75，扩容每次变为之前的2n+1，或者直接使用传入的capacity参数作为容量；
d.HashMap计算存储地址使用hash函数对key的hashcode()进行计算，再用IndexFor()计算地址；
HashTable直接使用key的(hashcode()&0x7FFFFFFF)%tab.length简单的取模计算得出index，其中0x7FFFFFFF是十六进制整型中的最大值。
e.HashMap继承于AbstractMap，而HashTable继承于旧类Dictionary。

10.Java 中的另一个线程安全的与 HashMap 极其类似的类是什么？同样是线程安全，它与 HashTable 在线程同步上有什么不同？
答：ConcurrentHashMap 类（是 Java并发包 java.util.concurrent 中提供的一个线程安全且高效的 HashMap 实现）。
HashTable 是使用 synchronize 关键字加锁的原理（就是对对象加锁）；
而针对 ConcurrentHashMap，在 JDK 1.7 中采用 分段锁的方式；JDK 1.8 中直接采用了CAS（无锁算法）+ synchronized。


